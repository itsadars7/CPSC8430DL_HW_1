{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60fbf7b2-25f6-4840-8abb-506ffef3c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os, math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b71df378-7dbc-4145-801f-082b880bf6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"HW_1-2-MinimalRatio\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42ed4be-c367-4b7f-8f99-750d108cf107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available(): return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee793b1d-8e8c-4e38-9d0b-2e1327046f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7dbff97-ff99-405f-b4b9-5c9f9f97f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Single-input Single-output function\n",
    "def f_true(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.cos(2* math.pi*x) * (x**3)\n",
    "\n",
    "def make_function_loaders(xmin=-3.0, xmax=3.0, n_train=256, n_eval=800, batch=128):\n",
    "    x_tr = torch.linspace(xmin, xmax, n_train).unsqueeze(1)\n",
    "    y_tr = f_true(x_tr)\n",
    "    x_ev = torch.linspace(xmin, xmax, n_eval).unsqueeze(1)\n",
    "    y_ev = f_true(x_ev)\n",
    "    train = DataLoader(TensorDataset(x_tr, y_tr), batch_size=batch, shuffle=True, drop_last=False)\n",
    "    eval_loader = DataLoader(TensorDataset(x_ev, y_ev), batch_size=256, shuffle=False)\n",
    "    return train, eval_loader\n",
    "\n",
    "class SimpleFunctionModel(nn.Module):\n",
    "    def __init__(self, hidden=[18, 20, 15]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_d = 1\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(in_d, h), nn.Tanh()]\n",
    "            in_d = h\n",
    "        layers += [nn.Linear(in_d, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1adfb6a2-0eb9-483e-8de0-d1701f486980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_channel_stats(data_dir=\"./data\"):\n",
    "    \"\"\"\n",
    "    Compute mean and std of CIFAR-10 training set.\n",
    "    Returns two lists: mean, std (each of length 3 for RGB).\n",
    "    \"\"\"\n",
    "    # Load train set\n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True,\n",
    "        transform=T.ToTensor()\n",
    "    )\n",
    "    loader = DataLoader(train_set, batch_size=5000, shuffle=False, num_workers=2)\n",
    "\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0\n",
    "\n",
    "    for data, _ in loader:\n",
    "        # data shape: [batch, channels, height, width]\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)  # flatten H*W\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std  += data.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "\n",
    "    return mean.tolist(), std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "658bb2ee-d6e7-4010-be5a-c5d091b0e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10_loaders(\n",
    "    data_dir=\"./data\",\n",
    "    batch_size=128,\n",
    "    subset_train: int = 5000, \n",
    "    subset_eval: int = 2000, \n",
    "    num_workers=2,\n",
    "    drop_last=False\n",
    "):\n",
    "    # compute mean/std\n",
    "    mean, std = compute_channel_stats(data_dir)\n",
    "    print(\"CIFAR-10 stats:\", mean, std)\n",
    "\n",
    "    train_tfms = T.Compose([\n",
    "        T.RandomCrop(32, padding=2),\n",
    "        T.RandomHorizontalFlip(), \n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tfms = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    full_train = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=train_tfms\n",
    "    )\n",
    "    full_test = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=test_tfms\n",
    "    )\n",
    "\n",
    "    idx_tr = list(range(min(subset_train, len(full_train))))\n",
    "    idx_ev = list(range(min(subset_eval, len(full_test))))\n",
    "    train_loader = DataLoader(Subset(full_train, idx_tr), batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(Subset(full_test,  idx_ev), batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "   \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa15ce5-5012-4ab9-8d01-afd4f096ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(inplace=True),   # <-- 'features.0' is first conv\n",
    "            nn.MaxPool2d(2),  # 32x16x16\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 64x8x8\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*8*8, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "    def forward(self, x): return self.classifier(self.features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c390f9d-13a5-4c4d-8baf-56f9ee3767f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def grad_norm(model: nn.Module) -> float:\n",
    "    grad_all = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grad_all += (p.grad.detach() ** 2).sum().item()\n",
    "    grad_norm = grad_all ** 0.5\n",
    "    return grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0201286-08ae-4618-8274-33e876d541c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_average_loss(model: nn.Module, loss_fn, loader: DataLoader, device: torch.device) -> float:\n",
    "    model.eval()\n",
    "    total, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            total += loss_fn(pred, yb).item() * xb.size(0)\n",
    "            n += xb.size(0)\n",
    "    return total / max(1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fed535a-93b2-45d9-a450-27d88f2f6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_norm_sq_from_loss(loss: torch.Tensor, params: List[torch.nn.Parameter]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute sum of squared gradients of `loss` wrt `params`.\n",
    "    \"\"\"\n",
    "    grads = torch.autograd.grad(loss, params, create_graph=True, retain_graph=True, allow_unused=True)\n",
    "    terms = []\n",
    "    for g in grads:\n",
    "        if g is not None:\n",
    "            terms.append((g**2).sum())\n",
    "    return torch.stack(terms).sum() if terms else loss*0.0  # safe zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3296a14-c02f-4da9-be45-77a1799f88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_ratio_sampling(model: nn.Module,\n",
    "                           loss_fn,\n",
    "                           eval_loader: DataLoader,\n",
    "                           device: torch.device,\n",
    "                           noise_std: float = 1e-3,\n",
    "                           samples: int = 50) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Sampling-based minimal ratio around current weights.\n",
    "    Returns (base_loss, ratio).\n",
    "    \"\"\"\n",
    "    base_loss = eval_average_loss(model, loss_fn, eval_loader, device)\n",
    "\n",
    "    # snapshot current params\n",
    "    base_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    worse = 0\n",
    "    total = 0\n",
    "\n",
    "    for _ in range(samples):\n",
    "        # perturb\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters():\n",
    "                p.add_(torch.randn_like(p) * noise_std)\n",
    "\n",
    "        loss_p = eval_average_loss(model, loss_fn, eval_loader, device)\n",
    "        total += 1\n",
    "        if loss_p > base_loss:  \n",
    "            worse += 1\n",
    "\n",
    "        # restore\n",
    "        model.load_state_dict(base_state, strict=True)\n",
    "\n",
    "    ratio = worse / max(1, total)\n",
    "    return base_loss, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df7885f8-06bd-46e5-9994-9e8fc8275335",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    runs: int = 100\n",
    "    stage1_epochs: int = 20        # task loss epochs\n",
    "    stage2_steps: int = 50         # grad-norm objective steps\n",
    "    stage2_tol: float = 1e-6       # stop if grad-norm^2 < tol\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.0\n",
    "    batch: int = 128\n",
    "    noise_std: float = 1e-3        # for sampling perturbations\n",
    "    samples: int = 50              # # of perturbations per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f669ffd-f7e2-4ff3-a477-5a1ea540fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reach_stationary_point(model: nn.Module,\n",
    "                           train_loader: DataLoader,\n",
    "                           loss_fn,\n",
    "                           cfg: Config,\n",
    "                           device: torch.device):\n",
    "    \"\"\"\n",
    "    Two-stage optimization:\n",
    "      Stage-1: minimize task loss.\n",
    "      Stage-2: minimize gradient-norm\n",
    "    Returns a model at (near) stationary point.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    # training on task loss\n",
    "    for _ in range(cfg.stage1_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    # minimize grad-norm squared of the same loss\n",
    "    # We keep using batches from the same train_loader\n",
    "    for step in range(cfg.stage2_steps):\n",
    "        model.train()\n",
    "        total_g2 = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            g2 = grad_norm_sq_from_loss(loss, list(model.parameters()))  # ∑||∇θ L||^2\n",
    "            g2.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_g2 += g2.detach().item()\n",
    "\n",
    "        # early stop if gradient norm is tiny (average over batches)\n",
    "        avg_g2 = total_g2 / max(1, len(train_loader))\n",
    "        if avg_g2 < cfg.stage2_tol:\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "065a39ca-d5c3-4ce7-ba65-90bd0a45b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task_FUNCTION(cfg: Config):\n",
    "    dev = get_device()\n",
    "    set_seed(42)\n",
    "\n",
    "    train_loader, eval_loader = make_function_loaders(batch=cfg.batch)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    losses, ratios = [], []\n",
    "    for r in range(cfg.runs):\n",
    "        set_seed(1000 + r)\n",
    "        model = SimpleFunctionModel()\n",
    "        model = reach_stationary_point(model, train_loader, loss_fn, cfg, dev)\n",
    "        base_loss, ratio = minimal_ratio_sampling(model, loss_fn, eval_loader, dev,\n",
    "                                                  noise_std=cfg.noise_std, samples=cfg.samples)\n",
    "        losses.append(base_loss); ratios.append(ratio)\n",
    "\n",
    "    # Plot loss vs minimal ratio\n",
    "    plt.figure(figsize=(6.8, 5.2))\n",
    "    plt.scatter(ratios, losses, s=20)\n",
    "    plt.xlabel(\"minimal ratio\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"FUNCTION: minimal ratio vs loss\")\n",
    "    plt.grid(True, linewidth=0.3)\n",
    "    out = f\"{OUT_DIR}/function_minratio_vs_loss.png\"\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=180); plt.close()\n",
    "    print(f\"[FUNCTION] saved: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3419b893-6c61-46ee-9ee1-e124feefe02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task_CIFAR(cfg: Config, data_dir=\"./data\"):\n",
    "    dev = get_device()\n",
    "    set_seed(42)\n",
    "\n",
    "    train_loader, eval_loader = get_cifar10_loaders(data_dir=data_dir, batch_size=cfg.batch,\n",
    "                                                  subset_train=5000, subset_eval=2000)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses, ratios = [], []\n",
    "    for r in range(cfg.runs):\n",
    "        set_seed(2000 + r)\n",
    "        model = CNNModel()\n",
    "        model = reach_stationary_point(model, train_loader, loss_fn, cfg, dev)\n",
    "        base_loss, ratio = minimal_ratio_sampling(model, loss_fn, eval_loader, dev,\n",
    "                                                  noise_std=cfg.noise_std, samples=cfg.samples)\n",
    "        losses.append(base_loss); ratios.append(ratio)\n",
    "\n",
    "    plt.figure(figsize=(6.8, 5.2))\n",
    "    plt.scatter(ratios, losses, s=20)\n",
    "    plt.xlabel(\"minimal ratio\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"CIFAR-10: minimal ratio vs loss (subset)\")\n",
    "    plt.grid(True, linewidth=0.3)\n",
    "    out = f\"{OUT_DIR}/cifar_minratio_vs_loss.png\"\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=180); plt.close()\n",
    "    print(f\"[CIFAR] saved: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5872dfbf-4b04-4b53-8499-376de23d0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(task=\"both\",\n",
    "         runs=100,\n",
    "         # Stage-1 (= train on task loss) epochs\n",
    "         task_loss_epochs=10,\n",
    "         # Stage-2 (= minimize grad-norm^2) steps\n",
    "         grad_norm_steps=30,\n",
    "         lr=1e-3, weight_decay=0.0,\n",
    "         batch=128,\n",
    "         noise_std=1e-3, samples=50,\n",
    "         data_dir=\"./data\"):\n",
    "    \"\"\"\n",
    "    Run the 'minimal ratio vs loss' experiment.\n",
    "\n",
    "    task: \"function\", \"cifar\", or \"both\"\n",
    "    runs: number of independent runs (100)\n",
    "    _epochs: epochs for stage-1 (task loss)\n",
    "    _steps:  steps for stage-2 (grad-norm^2 objective)\n",
    "    noise_std: perturbation std for minimal-ratio sampling\n",
    "    samples:   number of perturbation samples per run\n",
    "    \"\"\"\n",
    "    # build per-task configs\n",
    "    cfg_func = Config(\n",
    "        runs=runs, stage1_epochs=task_loss_epochs, stage2_steps=grad_norm_steps,\n",
    "        stage2_tol=1e-6, lr=lr, weight_decay=weight_decay,\n",
    "        batch=batch, noise_std=noise_std, samples=samples\n",
    "    )\n",
    "    cfg_cifar = Config(\n",
    "        runs=runs, stage1_epochs=task_loss_epochs, stage2_steps=grad_norm_steps,\n",
    "        stage2_tol=1e-6, lr=lr, weight_decay=weight_decay,\n",
    "        batch=batch, noise_std=noise_std, samples=samples\n",
    "    )\n",
    "\n",
    "    if task in (\"function\", \"both\"):\n",
    "        run_task_FUNCTION(cfg_func)\n",
    "\n",
    "    if task in (\"cifar\", \"both\"):\n",
    "        run_task_CIFAR(cfg_cifar, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61740828-73f2-4476-9027-0eaacc2efb69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FUNCTION] saved: HW_1-2-MinimalRatio/function_minratio_vs_loss.png\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adarshn/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 stats: [0.4913996756076813, 0.4821583926677704, 0.44653093814849854] [0.20230092108249664, 0.19941280782222748, 0.20096160471439362]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Function\n",
    "    main(task=\"function\")\n",
    "\n",
    "    # CIFAR\n",
    "    main(task=\"cifar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

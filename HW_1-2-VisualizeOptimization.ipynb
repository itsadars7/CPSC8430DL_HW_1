{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa62cf6-09cf-4c79-a86a-f0d1d1d7b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc66cfc6-b496-4d66-a3e7-de5b9e7d8b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d31777f-b674-4e22-b09b-35c0029de8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(model: nn.Module, layer_name: str) -> torch.Tensor:\n",
    "    \"\"\"Flatten a specific layer by name.\"\"\"\n",
    "    mod = model\n",
    "    for tok in layer_name.split('.'):\n",
    "        mod = getattr(mod, tok)\n",
    "    parts = [mod.weight.detach().flatten()]\n",
    "    if getattr(mod, 'bias', None) is not None:\n",
    "        parts.append(mod.bias.detach().flatten())\n",
    "    return torch.cat(parts).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f3c7d3-f7a8-4ed0-97bb-77094783cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_whole(model: nn.Module) -> torch.Tensor:\n",
    "    return torch.cat([p.detach().flatten() for p in model.parameters()]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963940d7-8385-48fd-89a5-f7ba3466e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca2d(X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    PCA to 2D via SVD. X: [N, D]. Returns Y: [N, 2].\n",
    "    \"\"\"\n",
    "    X = X - X.mean(dim=0, keepdim=True) # center the data\n",
    "\n",
    "    U, S, Vh = torch.linalg.svd(X, full_matrices=False)\n",
    "    W = Vh[:2].T   # picking top-2 principal directions\n",
    "    Y = X @ W      # projecting data onto these 2 principal directions\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b07feb7-1e7a-4d10-914c-0b4a1ab5abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Single-input Single-output function\n",
    "def f_true(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.cos(2* math.pi*x) * (x**3)\n",
    "\n",
    "def make_function_loaders(xmin=-3.0, xmax=3.0, n_train=256, batch=64, device=None):\n",
    "    x = torch.linspace(xmin, xmax, n_train).unsqueeze(1)\n",
    "    y = f_true(x)\n",
    "    ds = TensorDataset(x, y)\n",
    "    loader = DataLoader(ds, batch_size=batch, shuffle=True, drop_last=False)\n",
    "    if device:\n",
    "        pass\n",
    "    return loader\n",
    "\n",
    "class SimpleFunctionModel(nn.Module):\n",
    "    def __init__(self, hidden=[18, 20, 15]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_d = 1\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(in_d, h), nn.Tanh()]\n",
    "            in_d = h\n",
    "        layers += [nn.Linear(in_d, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c47d5bb-5bf0-42b2-afcd-ab0aa232c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_channel_stats(data_dir=\"./data\"):\n",
    "    \"\"\"\n",
    "    Compute mean and std of CIFAR-10 training set.\n",
    "    Returns two lists: mean, std (each of length 3 for RGB).\n",
    "    \"\"\"\n",
    "    # Load train set\n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True,\n",
    "        transform=T.ToTensor()\n",
    "    )\n",
    "    loader = DataLoader(train_set, batch_size=5000, shuffle=False, num_workers=2)\n",
    "\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0\n",
    "\n",
    "    for data, _ in loader:\n",
    "        # data shape: [batch, channels, height, width]\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)  # flatten H*W\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std  += data.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "\n",
    "    return mean.tolist(), std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86e2237-84d0-49a4-b5d2-3fe7a2db6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10_loaders(\n",
    "    data_dir=\"./data\",\n",
    "    batch_size=128,\n",
    "    num_workers=2,\n",
    "    drop_last=False\n",
    "):\n",
    "    # compute mean/std\n",
    "    mean, std = compute_channel_stats(data_dir)\n",
    "    print(\"CIFAR-10 stats:\", mean, std)\n",
    "\n",
    "    train_tfms = T.Compose([\n",
    "        T.RandomCrop(32, padding=2),\n",
    "        T.RandomHorizontalFlip(), \n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tfms = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=train_tfms\n",
    "    )\n",
    "    test_set = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=test_tfms\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, pin_memory=True, drop_last=drop_last\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True, drop_last=False\n",
    "    )\n",
    "    return train_loader, test_loader, train_set.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da792e8c-4aff-4028-9940-8120d8f2f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(inplace=True),   # <-- 'features.0' is first conv\n",
    "            nn.MaxPool2d(2),  # 32x16x16\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 64x8x8\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*8*8, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "    def forward(self, x): return self.classifier(self.features(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "932b2df1-a758-48ec-9cba-d1ccd64124f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(task=\"function\"):\n",
    "    if task == \"function\":\n",
    "        # single-input single-output generated data\n",
    "        return train_loader, test_loader, input_dim, output_dim\n",
    "    elif task == \"cifar\":\n",
    "        # CIFAR-10 dataset\n",
    "        return train_loader, test_loader, input_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6ea978d-036b-405b-a40d-b6310b616428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(task=\"function\"):\n",
    "    if task == \"function\":\n",
    "        return SimpleFunctionModel(hidden_sizes=[64,64])\n",
    "    elif task == \"cifar\":\n",
    "        return CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d953f6bd-2396-496b-91dd-b07a422ae43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj(xy: torch.Tensor, runs: int, pts_per_run: int, title: str, out_path: str, epoch_marks: List[int]):\n",
    "    \"\"\"Plot PCA trajectories for all runs.\"\"\"\n",
    "    plt.figure(figsize=(7.2, 5.6))\n",
    "    cmap = plt.cm.viridis\n",
    "    colors = cmap(np.linspace(0, 1, pts_per_run))\n",
    "    for r in range(runs):\n",
    "        s, e = r*pts_per_run, (r+1)*pts_per_run\n",
    "        coords = xy[s:e].numpy()\n",
    "        plt.plot(coords[:,0], coords[:,1], '-', alpha=0.7, linewidth=2)\n",
    "        for i in range(pts_per_run):\n",
    "            plt.scatter(coords[i,0], coords[i,1], s=50, c=colors[i].reshape(1, -1))\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=epoch_marks[0], vmax=epoch_marks[-1]))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, pad=0.01)\n",
    "    cbar.set_label(\"Epoch\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PCA-1\"); plt.ylabel(\"PCA-2\")\n",
    "    plt.grid(True, linewidth=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=180)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b5f7c9e-b470-4ab2-9c31-c5e3142c913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_function(model, loader, opt, device):\n",
    "    model.train()\n",
    "    loss_fn = nn.MSELoss() # MSE loss function for single-input single-output function\n",
    "    total_loss = 0.0\n",
    "    total = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        total += xb.size(0)\n",
    "    return total_loss / total  \n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_cifar(model, loader, device):\n",
    "    model.eval()\n",
    "    total, correct, tot_loss = 0, 0, 0.0\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum') # Cross entropy loss function for CIFAR dataset\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        tot_loss += loss_fn(logits, yb).item()\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return tot_loss/total, correct/total\n",
    "\n",
    "def train_epoch_cifar(model, loader, opt, device):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    total, correct, tot_loss = 0, 0, 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        tot_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return tot_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "826540da-ee80-4b0a-9632-56ea345f939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one experiment (task flag)\n",
    "def run_experiment(task: str,\n",
    "                   runs: int = 8,\n",
    "                   epochs: int = 12,\n",
    "                   log_every: int = 3,\n",
    "                   lr: float = 1e-3,\n",
    "                   weight_decay: float = 5e-4,\n",
    "                   batch_size: int = 128,\n",
    "                   data_dir: str = \"./data\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\n=== Task: {task} | device: {device} ===\")\n",
    "\n",
    "    epoch_marks = list(range(0, epochs+1, log_every))  \n",
    "    pts_per_run = len(epoch_marks)\n",
    "\n",
    "    # storage across runs\n",
    "    layer_trajs: List[List[torch.Tensor]] = []\n",
    "    whole_trajs: List[List[torch.Tensor]] = []\n",
    "    metric_logs: List[List[float]] = []  # loss\n",
    "\n",
    "    if task == \"function\":\n",
    "        train_loader = make_function_loaders(batch=batch_size)\n",
    "        layer_name = \"net.0\"  # first Linear\n",
    "        for r in range(runs):\n",
    "            set_seed(1000 + r)\n",
    "            model = SimpleFunctionModel(hidden=[64, 64]).to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "            layer_ckpts = [flatten_layer(model, layer_name)]\n",
    "            whole_ckpts = [flatten_whole(model)]\n",
    "            metric_ckpt = []  \n",
    "\n",
    "            for ep in range(1, epochs+1):\n",
    "                tr_loss = train_epoch_function(model, train_loader, opt, device)\n",
    "                if ep % log_every == 0:\n",
    "                    metric_ckpt.append(tr_loss)\n",
    "                    layer_ckpts.append(flatten_layer(model, layer_name))\n",
    "                    whole_ckpts.append(flatten_whole(model))\n",
    "                # print(f\"[Run {r+1}/{runs}] Epoch {ep:02d}/{epochs} | TrainLoss {tr_loss:.4f}\")\n",
    "\n",
    "            layer_trajs.append(layer_ckpts)\n",
    "            whole_trajs.append(whole_ckpts)\n",
    "            metric_logs.append(metric_ckpt)\n",
    "\n",
    "    elif task == \"cifar\":\n",
    "        train_loader, test_loader, _ = get_cifar10_loaders(data_dir=data_dir, batch_size=batch_size)\n",
    "        layer_name = \"features.0\"  # first Conv2d\n",
    "        for r in range(runs):\n",
    "            set_seed(2000 + r)\n",
    "            model = CNNModel().to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "            layer_ckpts = [flatten_layer(model, layer_name)]\n",
    "            whole_ckpts = [flatten_whole(model)]\n",
    "            metric_ckpt = []  \n",
    "\n",
    "            for ep in range(1, epochs+1):\n",
    "                tr_loss, tr_acc = train_epoch_cifar(model, train_loader, opt, device)\n",
    "                if ep % log_every == 0:\n",
    "                    metric_ckpt.append(tr_loss)  \n",
    "                    layer_ckpts.append(flatten_layer(model, layer_name))\n",
    "                    whole_ckpts.append(flatten_whole(model))\n",
    "                # print(f\"[Run {r+1}/{runs}] Epoch {ep:02d}/{epochs} | TrainLoss {tr_loss:.4f} | TrainAcc {tr_acc*100:5.2f}%\")\n",
    "\n",
    "            layer_trajs.append(layer_ckpts)\n",
    "            whole_trajs.append(whole_ckpts)\n",
    "            metric_logs.append(metric_ckpt)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"task must be 'function' or 'cifar'\")\n",
    "        \n",
    "    # PCA fit on ALL checkpoints across ALL runs\n",
    "    layer_mat = torch.stack([v for run in layer_trajs for v in run], dim=0)  \n",
    "    whole_mat = torch.stack([v for run in whole_trajs for v in run], dim=0)\n",
    "    layer_xy = pca2d(layer_mat)\n",
    "    whole_xy = pca2d(whole_mat)\n",
    "\n",
    "    # Plots\n",
    "    plot_traj(layer_xy, runs, pts_per_run,\n",
    "              f\"Optimization Trajectories (First Layer) [{task}]\",\n",
    "              f\"HW_1-2-VisualizeOptimization/opt_traj_layer_{task}.png\",\n",
    "              epoch_marks)\n",
    "    plot_traj(whole_xy, runs, pts_per_run,\n",
    "              f\"Optimization Trajectories (Whole model) [{task}]\",\n",
    "              f\"HW_1-2-VisualizeOptimization/opt_traj_whole_{task}.png\",\n",
    "              epoch_marks)\n",
    "\n",
    "    print(\"\\nSaved figures:\")\n",
    "    print(f\" - HW_1-2-VisualizeOptimization/opt_traj_layer_{task}.png\")\n",
    "    print(f\" - HW_1-2-VisualizeOptimization/opt_traj_whole_{task}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fd2fc09-7cf2-40cc-bd21-7b353d85a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(task=\"both\",\n",
    "                 runs=8,\n",
    "                 epochs=12,\n",
    "                 log_every=3,\n",
    "                 lr=1e-3,\n",
    "                 weight_decay=5e-4,\n",
    "                 batch_size=128,\n",
    "                 data_dir=\"./data\"):\n",
    "\n",
    "    if task in (\"function\", \"both\"):\n",
    "        run_experiment(\"function\", runs=runs, epochs=epochs, log_every=log_every,\n",
    "                       lr=lr, weight_decay=weight_decay, batch_size=batch_size,\n",
    "                       data_dir=data_dir)\n",
    "\n",
    "    if task in (\"cifar\", \"both\"):\n",
    "        run_experiment(\"cifar\", runs=runs, epochs=epochs, log_every=log_every,\n",
    "                       lr=lr, weight_decay=weight_decay, batch_size=batch_size,\n",
    "                       data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e5bbf5-74ef-4f69-be05-4a7a0eaf1df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Task: function | device: cuda ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.5507703/ipykernel_3701641/3021631220.py:14: MatplotlibDeprecationWarning: Unable to determine Axes to steal space for Colorbar. Using gca(), but will raise in the future. Either provide the *cax* argument to use as the Axes for the Colorbar, provide the *ax* argument to steal space from it, or add *mappable* to an Axes.\n",
      "  cbar = plt.colorbar(sm, pad=0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved figures:\n",
      " - HW_1-2-VisualizeOptimization/opt_traj_layer_function.png\n",
      " - HW_1-2-VisualizeOptimization/opt_traj_whole_function.png\n",
      "\n",
      "=== Task: cifar | device: cuda ===\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adarshn/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 stats: [0.4913996756076813, 0.4821583926677704, 0.44653093814849854] [0.20230092108249664, 0.19941280782222748, 0.20096160471439362]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(task=\"function\", epochs=1000)\n",
    "    main(task=\"cifar\", epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe566203-c49c-4b9c-80a7-edc16779c9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

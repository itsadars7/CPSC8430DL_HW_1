{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50752fc5-550b-4e2d-a560-ad0f9b4bf554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "badb760e-45e0-4e09-a56b-f1cf6be3c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd00e83-75f7-4c31-8561-0f47f26e3463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_channel_stats(data_dir=\"./data\"):\n",
    "    \"\"\"\n",
    "    Compute mean and std of CIFAR-10 training set.\n",
    "    Returns two lists: mean, std (each of length 3 for RGB).\n",
    "    \"\"\"\n",
    "    # Load train set\n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True,\n",
    "        transform=T.ToTensor()\n",
    "    )\n",
    "    loader = DataLoader(train_set, batch_size=5000, shuffle=False, num_workers=2)\n",
    "\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0\n",
    "\n",
    "    for data, _ in loader:\n",
    "        # data shape: [batch, channels, height, width]\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)  # flatten H*W\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std  += data.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "\n",
    "    return mean.tolist(), std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1401d59d-e947-40fa-97b9-94069c41b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10_loaders(\n",
    "    data_dir=\"./data\",\n",
    "    batch_size=128,\n",
    "    num_workers=2,\n",
    "    drop_last=False\n",
    "):\n",
    "    # compute mean/std\n",
    "    mean, std = compute_channel_stats(data_dir)\n",
    "    print(\"CIFAR-10 stats:\", mean, std)\n",
    "\n",
    "    train_tfms = T.Compose([\n",
    "        T.RandomCrop(32, padding=2),\n",
    "        T.RandomHorizontalFlip(), \n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tfms = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=train_tfms\n",
    "    )\n",
    "    test_set = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=test_tfms\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, pin_memory=True, drop_last=drop_last\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True, drop_last=False\n",
    "    )\n",
    "    return train_loader, test_loader, train_set.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd85c40-3451-4ce8-af3e-4d06df0e2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN model with less parameters\n",
    "class CNN1(nn.Module):\n",
    "    \"\"\"\n",
    "    Two conv blocks + 2x2 pools → 8x8 feature map.\n",
    "    Head: 4096 → 128 → 10\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 32x16x16\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 64x8x8\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*8*8, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "    def forward(self, x): return self.classifier(self.features(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8175c78d-0f16-492e-b6e7-6b656572e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medium CNN model with parameters in medium range\n",
    "class CNN2(nn.Module):\n",
    "    \"\"\"\n",
    "    Two blocks with doubled channels and BN:\n",
    "    Block1: 3→64→64 → pool (16x16)\n",
    "    Block2: 64→128→128 → pool (8x8)\n",
    "    Head: 8192 → 256 → 10\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 64x16x16\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 128x8x8\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*8*8, 256), nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "    def forward(self, x): return self.classifier(self.features(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59fc0358-7895-4d68-8de4-73065d5ddfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex CNN model with high number of parameters\n",
    "class CNN3(nn.Module):\n",
    "    \"\"\"\n",
    "    Three blocks with more channels:\n",
    "    B1: 3→64→64 → pool (16x16)\n",
    "    B2: 64→128→128 → pool (8x8)\n",
    "    B3: 128→256→256 → pool (4x4)\n",
    "    Head: 4096 → 512 → 10\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 64x16x16\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 128x8x8\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 256x4x4\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*4*4, 512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "    def forward(self, x): return self.classifier(self.features(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b84d08ae-9166-4d0b-bf28-7cd533a4023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f27871b-4b12-432d-8205-ea721f0dec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return total_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b23b2849-d4cf-4469-913c-022e815dd4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(history_dict, epochs, ylabel, title, out_path):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    for name, values in history_dict.items():\n",
    "        plt.plot(range(1, epochs+1), values, label=name, linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linewidth=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=180)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eb6ba56-8353-44ad-8519-27fc1185186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    seed=42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE   = 128\n",
    "    EPOCHS       = 100\n",
    "    LR           = 1e-3\n",
    "    WEIGHT_DECAY = 5e-4  \n",
    "\n",
    "    # Data\n",
    "    train_loader, test_loader, class_names = get_cifar10_loaders(\n",
    "        data_dir=\"./data\", batch_size=BATCH_SIZE, num_workers=2, drop_last=False\n",
    "    )\n",
    "    print(\"Classes:\", class_names)\n",
    "\n",
    "    # Models\n",
    "    models = {\n",
    "        \"CNN Model 1\":  CNN1().to(device),\n",
    "        \"CNN Model 2\": CNN2().to(device),\n",
    "        \"CNN Model 3\":  CNN3().to(device),\n",
    "    }\n",
    "    for name, m in models.items():\n",
    "        print(f\"{name}: {count_params(m):,} params\")\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss() # Cross Entropy is used as the loss function\n",
    "\n",
    "    # Histories\n",
    "    train_losses = {k: [] for k in models.keys()}\n",
    "    train_accs   = {k: [] for k in models.keys()}\n",
    "\n",
    "    # Train all models\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n     Training {name} Model\\n\")\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        for epoch in range(1, EPOCHS+1):\n",
    "            tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "            train_losses[name].append(tr_loss)\n",
    "            train_accs[name].append(tr_acc)\n",
    "\n",
    "            print(f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "                  f\"Train Loss: {tr_loss:.4f} Acc: {tr_acc*100:5.2f}%\")\n",
    "\n",
    "    # training Loss and training Accuracy plots \n",
    "    plot_curves(train_losses, EPOCHS, ylabel=\"Training Loss\",\n",
    "                title=\"Model training loss\",\n",
    "                out_path=\"HW_1-1-TrainOnActualDataset/Task1/cifar_loss.png\")\n",
    "\n",
    "    plot_curves(train_accs, EPOCHS, ylabel=\"Training Accuracy\",\n",
    "                title=\"Model training accuracy\",\n",
    "                out_path=\"HW_1-1-TrainOnActualDataset/Task1/cifar_train_acc.png\")\n",
    "\n",
    "    print(\"\\nSaved figures:\")\n",
    "    print(\" - cifar_loss.png\")\n",
    "    print(\" - cifar_train_acc.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f110ca4-7e31-457b-91a5-32023585984c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:02<00:00, 57.9MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adarshn/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 stats: [0.4913996756076813, 0.4821583926677704, 0.44653093814849854] [0.20230092108249664, 0.19941280782222748, 0.20096160471439362]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "CNN Model 1: 545,098 params\n",
      "CNN Model 2: 2,360,906 params\n",
      "CNN Model 3: 3,249,994 params\n",
      "\n",
      "     Training CNN Model 1 Model\n",
      "\n",
      "Epoch 01/100 | Train Loss: 1.4411 Acc: 48.29%\n",
      "Epoch 02/100 | Train Loss: 1.0873 Acc: 61.83%\n",
      "Epoch 03/100 | Train Loss: 0.9805 Acc: 65.46%\n",
      "Epoch 04/100 | Train Loss: 0.9109 Acc: 68.11%\n",
      "Epoch 05/100 | Train Loss: 0.8625 Acc: 69.72%\n",
      "Epoch 06/100 | Train Loss: 0.8203 Acc: 71.37%\n",
      "Epoch 07/100 | Train Loss: 0.7852 Acc: 72.79%\n",
      "Epoch 08/100 | Train Loss: 0.7564 Acc: 73.66%\n",
      "Epoch 09/100 | Train Loss: 0.7389 Acc: 74.27%\n",
      "Epoch 10/100 | Train Loss: 0.7129 Acc: 75.18%\n",
      "Epoch 11/100 | Train Loss: 0.6980 Acc: 75.71%\n",
      "Epoch 12/100 | Train Loss: 0.6775 Acc: 76.32%\n",
      "Epoch 13/100 | Train Loss: 0.6659 Acc: 76.89%\n",
      "Epoch 14/100 | Train Loss: 0.6521 Acc: 77.18%\n",
      "Epoch 15/100 | Train Loss: 0.6409 Acc: 77.62%\n",
      "Epoch 16/100 | Train Loss: 0.6302 Acc: 77.96%\n",
      "Epoch 17/100 | Train Loss: 0.6145 Acc: 78.39%\n",
      "Epoch 18/100 | Train Loss: 0.6096 Acc: 78.60%\n",
      "Epoch 19/100 | Train Loss: 0.6018 Acc: 78.81%\n",
      "Epoch 20/100 | Train Loss: 0.5902 Acc: 79.21%\n",
      "Epoch 21/100 | Train Loss: 0.5858 Acc: 79.57%\n",
      "Epoch 22/100 | Train Loss: 0.5733 Acc: 79.85%\n",
      "Epoch 23/100 | Train Loss: 0.5699 Acc: 80.12%\n",
      "Epoch 24/100 | Train Loss: 0.5642 Acc: 80.31%\n",
      "Epoch 25/100 | Train Loss: 0.5584 Acc: 80.34%\n",
      "Epoch 26/100 | Train Loss: 0.5483 Acc: 80.81%\n",
      "Epoch 27/100 | Train Loss: 0.5492 Acc: 80.76%\n",
      "Epoch 28/100 | Train Loss: 0.5417 Acc: 81.02%\n",
      "Epoch 29/100 | Train Loss: 0.5297 Acc: 81.58%\n",
      "Epoch 30/100 | Train Loss: 0.5349 Acc: 81.28%\n",
      "Epoch 31/100 | Train Loss: 0.5270 Acc: 81.47%\n",
      "Epoch 32/100 | Train Loss: 0.5233 Acc: 81.63%\n",
      "Epoch 33/100 | Train Loss: 0.5227 Acc: 81.72%\n",
      "Epoch 34/100 | Train Loss: 0.5154 Acc: 82.01%\n",
      "Epoch 35/100 | Train Loss: 0.5103 Acc: 82.14%\n",
      "Epoch 36/100 | Train Loss: 0.5089 Acc: 82.10%\n",
      "Epoch 37/100 | Train Loss: 0.5031 Acc: 82.07%\n",
      "Epoch 38/100 | Train Loss: 0.5061 Acc: 82.38%\n",
      "Epoch 39/100 | Train Loss: 0.5009 Acc: 82.29%\n",
      "Epoch 40/100 | Train Loss: 0.4967 Acc: 82.60%\n",
      "Epoch 41/100 | Train Loss: 0.4909 Acc: 82.83%\n",
      "Epoch 42/100 | Train Loss: 0.4908 Acc: 82.73%\n",
      "Epoch 43/100 | Train Loss: 0.4878 Acc: 82.81%\n",
      "Epoch 44/100 | Train Loss: 0.4844 Acc: 82.95%\n",
      "Epoch 45/100 | Train Loss: 0.4824 Acc: 83.20%\n",
      "Epoch 46/100 | Train Loss: 0.4771 Acc: 83.43%\n",
      "Epoch 47/100 | Train Loss: 0.4775 Acc: 83.34%\n",
      "Epoch 48/100 | Train Loss: 0.4719 Acc: 83.40%\n",
      "Epoch 49/100 | Train Loss: 0.4802 Acc: 83.14%\n",
      "Epoch 50/100 | Train Loss: 0.4735 Acc: 83.32%\n",
      "Epoch 51/100 | Train Loss: 0.4705 Acc: 83.44%\n",
      "Epoch 52/100 | Train Loss: 0.4712 Acc: 83.36%\n",
      "Epoch 53/100 | Train Loss: 0.4670 Acc: 83.54%\n",
      "Epoch 54/100 | Train Loss: 0.4638 Acc: 83.83%\n",
      "Epoch 55/100 | Train Loss: 0.4571 Acc: 84.00%\n",
      "Epoch 56/100 | Train Loss: 0.4592 Acc: 83.83%\n",
      "Epoch 57/100 | Train Loss: 0.4621 Acc: 83.83%\n",
      "Epoch 58/100 | Train Loss: 0.4529 Acc: 84.03%\n",
      "Epoch 59/100 | Train Loss: 0.4604 Acc: 83.93%\n",
      "Epoch 60/100 | Train Loss: 0.4487 Acc: 84.26%\n",
      "Epoch 61/100 | Train Loss: 0.4576 Acc: 84.04%\n",
      "Epoch 62/100 | Train Loss: 0.4513 Acc: 84.14%\n",
      "Epoch 63/100 | Train Loss: 0.4481 Acc: 84.09%\n",
      "Epoch 64/100 | Train Loss: 0.4493 Acc: 84.10%\n",
      "Epoch 65/100 | Train Loss: 0.4542 Acc: 83.94%\n",
      "Epoch 66/100 | Train Loss: 0.4465 Acc: 84.49%\n",
      "Epoch 67/100 | Train Loss: 0.4472 Acc: 84.37%\n",
      "Epoch 68/100 | Train Loss: 0.4454 Acc: 84.29%\n",
      "Epoch 69/100 | Train Loss: 0.4453 Acc: 84.39%\n",
      "Epoch 70/100 | Train Loss: 0.4461 Acc: 84.42%\n",
      "Epoch 71/100 | Train Loss: 0.4429 Acc: 84.41%\n",
      "Epoch 72/100 | Train Loss: 0.4390 Acc: 84.63%\n",
      "Epoch 73/100 | Train Loss: 0.4405 Acc: 84.44%\n",
      "Epoch 74/100 | Train Loss: 0.4352 Acc: 84.63%\n",
      "Epoch 75/100 | Train Loss: 0.4393 Acc: 84.42%\n",
      "Epoch 76/100 | Train Loss: 0.4352 Acc: 84.77%\n",
      "Epoch 77/100 | Train Loss: 0.4378 Acc: 84.69%\n",
      "Epoch 78/100 | Train Loss: 0.4280 Acc: 85.04%\n",
      "Epoch 79/100 | Train Loss: 0.4322 Acc: 84.88%\n",
      "Epoch 80/100 | Train Loss: 0.4363 Acc: 84.82%\n",
      "Epoch 81/100 | Train Loss: 0.4305 Acc: 84.98%\n",
      "Epoch 82/100 | Train Loss: 0.4342 Acc: 84.97%\n",
      "Epoch 83/100 | Train Loss: 0.4295 Acc: 84.89%\n",
      "Epoch 84/100 | Train Loss: 0.4261 Acc: 85.06%\n",
      "Epoch 85/100 | Train Loss: 0.4278 Acc: 84.81%\n",
      "Epoch 86/100 | Train Loss: 0.4250 Acc: 85.04%\n",
      "Epoch 87/100 | Train Loss: 0.4268 Acc: 85.07%\n",
      "Epoch 88/100 | Train Loss: 0.4284 Acc: 84.94%\n",
      "Epoch 89/100 | Train Loss: 0.4235 Acc: 85.26%\n",
      "Epoch 90/100 | Train Loss: 0.4216 Acc: 85.10%\n",
      "Epoch 91/100 | Train Loss: 0.4176 Acc: 85.32%\n",
      "Epoch 92/100 | Train Loss: 0.4217 Acc: 85.10%\n",
      "Epoch 93/100 | Train Loss: 0.4232 Acc: 85.19%\n",
      "Epoch 94/100 | Train Loss: 0.4227 Acc: 84.99%\n",
      "Epoch 95/100 | Train Loss: 0.4194 Acc: 85.32%\n",
      "Epoch 96/100 | Train Loss: 0.4175 Acc: 85.38%\n",
      "Epoch 97/100 | Train Loss: 0.4207 Acc: 85.33%\n",
      "Epoch 98/100 | Train Loss: 0.4145 Acc: 85.37%\n",
      "Epoch 99/100 | Train Loss: 0.4203 Acc: 85.16%\n",
      "Epoch 100/100 | Train Loss: 0.4203 Acc: 85.17%\n",
      "\n",
      "     Training CNN Model 2 Model\n",
      "\n",
      "Epoch 01/100 | Train Loss: 1.5167 Acc: 45.70%\n",
      "Epoch 02/100 | Train Loss: 0.9697 Acc: 65.50%\n",
      "Epoch 03/100 | Train Loss: 0.7875 Acc: 72.44%\n",
      "Epoch 04/100 | Train Loss: 0.6939 Acc: 75.74%\n",
      "Epoch 05/100 | Train Loss: 0.6292 Acc: 77.96%\n",
      "Epoch 06/100 | Train Loss: 0.5846 Acc: 79.66%\n",
      "Epoch 07/100 | Train Loss: 0.5452 Acc: 80.95%\n",
      "Epoch 08/100 | Train Loss: 0.5144 Acc: 82.17%\n",
      "Epoch 09/100 | Train Loss: 0.4850 Acc: 83.12%\n",
      "Epoch 10/100 | Train Loss: 0.4688 Acc: 83.61%\n",
      "Epoch 11/100 | Train Loss: 0.4499 Acc: 84.30%\n",
      "Epoch 12/100 | Train Loss: 0.4311 Acc: 84.81%\n",
      "Epoch 13/100 | Train Loss: 0.4117 Acc: 85.53%\n",
      "Epoch 14/100 | Train Loss: 0.3972 Acc: 86.08%\n",
      "Epoch 15/100 | Train Loss: 0.3815 Acc: 86.55%\n",
      "Epoch 16/100 | Train Loss: 0.3745 Acc: 86.96%\n",
      "Epoch 17/100 | Train Loss: 0.3634 Acc: 87.44%\n",
      "Epoch 18/100 | Train Loss: 0.3559 Acc: 87.62%\n",
      "Epoch 19/100 | Train Loss: 0.3485 Acc: 87.93%\n",
      "Epoch 20/100 | Train Loss: 0.3399 Acc: 88.08%\n",
      "Epoch 21/100 | Train Loss: 0.3338 Acc: 88.33%\n",
      "Epoch 22/100 | Train Loss: 0.3240 Acc: 88.64%\n",
      "Epoch 23/100 | Train Loss: 0.3224 Acc: 88.76%\n",
      "Epoch 24/100 | Train Loss: 0.3137 Acc: 89.14%\n",
      "Epoch 25/100 | Train Loss: 0.3095 Acc: 89.18%\n",
      "Epoch 26/100 | Train Loss: 0.2992 Acc: 89.45%\n",
      "Epoch 27/100 | Train Loss: 0.3009 Acc: 89.45%\n",
      "Epoch 28/100 | Train Loss: 0.2902 Acc: 89.83%\n",
      "Epoch 29/100 | Train Loss: 0.2888 Acc: 89.99%\n",
      "Epoch 30/100 | Train Loss: 0.2833 Acc: 90.16%\n",
      "Epoch 31/100 | Train Loss: 0.2846 Acc: 89.92%\n",
      "Epoch 32/100 | Train Loss: 0.2758 Acc: 90.38%\n",
      "Epoch 33/100 | Train Loss: 0.2769 Acc: 90.45%\n",
      "Epoch 34/100 | Train Loss: 0.2741 Acc: 90.34%\n",
      "Epoch 35/100 | Train Loss: 0.2717 Acc: 90.49%\n",
      "Epoch 36/100 | Train Loss: 0.2687 Acc: 90.64%\n",
      "Epoch 37/100 | Train Loss: 0.2567 Acc: 91.01%\n",
      "Epoch 38/100 | Train Loss: 0.2599 Acc: 90.86%\n",
      "Epoch 39/100 | Train Loss: 0.2570 Acc: 90.95%\n",
      "Epoch 40/100 | Train Loss: 0.2538 Acc: 91.19%\n",
      "Epoch 41/100 | Train Loss: 0.2491 Acc: 91.32%\n",
      "Epoch 42/100 | Train Loss: 0.2526 Acc: 91.13%\n",
      "Epoch 43/100 | Train Loss: 0.2476 Acc: 91.36%\n",
      "Epoch 44/100 | Train Loss: 0.2476 Acc: 91.35%\n",
      "Epoch 45/100 | Train Loss: 0.2438 Acc: 91.52%\n",
      "Epoch 46/100 | Train Loss: 0.2465 Acc: 91.35%\n",
      "Epoch 47/100 | Train Loss: 0.2402 Acc: 91.66%\n",
      "Epoch 48/100 | Train Loss: 0.2372 Acc: 91.76%\n",
      "Epoch 49/100 | Train Loss: 0.2381 Acc: 91.74%\n",
      "Epoch 50/100 | Train Loss: 0.2364 Acc: 91.68%\n",
      "Epoch 51/100 | Train Loss: 0.2321 Acc: 91.81%\n",
      "Epoch 52/100 | Train Loss: 0.2356 Acc: 91.79%\n",
      "Epoch 53/100 | Train Loss: 0.2342 Acc: 91.81%\n",
      "Epoch 54/100 | Train Loss: 0.2341 Acc: 91.94%\n",
      "Epoch 55/100 | Train Loss: 0.2298 Acc: 91.91%\n",
      "Epoch 56/100 | Train Loss: 0.2312 Acc: 91.92%\n",
      "Epoch 57/100 | Train Loss: 0.2270 Acc: 91.95%\n",
      "Epoch 58/100 | Train Loss: 0.2291 Acc: 91.97%\n",
      "Epoch 59/100 | Train Loss: 0.2283 Acc: 92.01%\n",
      "Epoch 60/100 | Train Loss: 0.2199 Acc: 92.38%\n",
      "Epoch 61/100 | Train Loss: 0.2250 Acc: 92.08%\n",
      "Epoch 62/100 | Train Loss: 0.2226 Acc: 92.12%\n",
      "Epoch 63/100 | Train Loss: 0.2178 Acc: 92.33%\n",
      "Epoch 64/100 | Train Loss: 0.2202 Acc: 92.24%\n",
      "Epoch 65/100 | Train Loss: 0.2222 Acc: 92.16%\n",
      "Epoch 66/100 | Train Loss: 0.2183 Acc: 92.36%\n",
      "Epoch 67/100 | Train Loss: 0.2203 Acc: 92.28%\n",
      "Epoch 68/100 | Train Loss: 0.2199 Acc: 92.34%\n",
      "Epoch 69/100 | Train Loss: 0.2167 Acc: 92.41%\n",
      "Epoch 70/100 | Train Loss: 0.2111 Acc: 92.61%\n",
      "Epoch 71/100 | Train Loss: 0.2147 Acc: 92.53%\n",
      "Epoch 72/100 | Train Loss: 0.2159 Acc: 92.36%\n",
      "Epoch 73/100 | Train Loss: 0.2135 Acc: 92.49%\n",
      "Epoch 74/100 | Train Loss: 0.2109 Acc: 92.61%\n",
      "Epoch 75/100 | Train Loss: 0.2127 Acc: 92.49%\n",
      "Epoch 76/100 | Train Loss: 0.2111 Acc: 92.52%\n",
      "Epoch 77/100 | Train Loss: 0.2066 Acc: 92.70%\n",
      "Epoch 78/100 | Train Loss: 0.2109 Acc: 92.64%\n",
      "Epoch 79/100 | Train Loss: 0.2121 Acc: 92.45%\n",
      "Epoch 80/100 | Train Loss: 0.2068 Acc: 92.85%\n",
      "Epoch 81/100 | Train Loss: 0.2038 Acc: 92.85%\n",
      "Epoch 82/100 | Train Loss: 0.2089 Acc: 92.69%\n",
      "Epoch 83/100 | Train Loss: 0.2031 Acc: 92.86%\n",
      "Epoch 84/100 | Train Loss: 0.2067 Acc: 92.77%\n",
      "Epoch 85/100 | Train Loss: 0.2076 Acc: 92.70%\n",
      "Epoch 86/100 | Train Loss: 0.2044 Acc: 92.87%\n",
      "Epoch 87/100 | Train Loss: 0.2126 Acc: 92.50%\n",
      "Epoch 88/100 | Train Loss: 0.2083 Acc: 92.86%\n",
      "Epoch 89/100 | Train Loss: 0.2000 Acc: 93.07%\n",
      "Epoch 90/100 | Train Loss: 0.2031 Acc: 92.85%\n",
      "Epoch 91/100 | Train Loss: 0.2084 Acc: 92.68%\n",
      "Epoch 92/100 | Train Loss: 0.2053 Acc: 92.79%\n",
      "Epoch 93/100 | Train Loss: 0.1981 Acc: 93.07%\n",
      "Epoch 94/100 | Train Loss: 0.2013 Acc: 93.03%\n",
      "Epoch 95/100 | Train Loss: 0.2039 Acc: 92.78%\n",
      "Epoch 96/100 | Train Loss: 0.2006 Acc: 93.00%\n",
      "Epoch 97/100 | Train Loss: 0.1978 Acc: 93.13%\n",
      "Epoch 98/100 | Train Loss: 0.2011 Acc: 92.99%\n",
      "Epoch 99/100 | Train Loss: 0.2034 Acc: 92.86%\n",
      "Epoch 100/100 | Train Loss: 0.2034 Acc: 92.77%\n",
      "\n",
      "     Training CNN Model 3 Model\n",
      "\n",
      "Epoch 01/100 | Train Loss: 1.5132 Acc: 44.59%\n",
      "Epoch 02/100 | Train Loss: 0.9500 Acc: 65.88%\n",
      "Epoch 03/100 | Train Loss: 0.7408 Acc: 74.02%\n",
      "Epoch 04/100 | Train Loss: 0.6298 Acc: 78.22%\n",
      "Epoch 05/100 | Train Loss: 0.5611 Acc: 80.41%\n",
      "Epoch 06/100 | Train Loss: 0.5093 Acc: 82.37%\n",
      "Epoch 07/100 | Train Loss: 0.4649 Acc: 83.94%\n",
      "Epoch 08/100 | Train Loss: 0.4354 Acc: 85.03%\n",
      "Epoch 09/100 | Train Loss: 0.4118 Acc: 85.83%\n",
      "Epoch 10/100 | Train Loss: 0.3806 Acc: 86.84%\n",
      "Epoch 11/100 | Train Loss: 0.3633 Acc: 87.52%\n",
      "Epoch 12/100 | Train Loss: 0.3505 Acc: 88.08%\n",
      "Epoch 13/100 | Train Loss: 0.3375 Acc: 88.36%\n",
      "Epoch 14/100 | Train Loss: 0.3237 Acc: 88.79%\n",
      "Epoch 15/100 | Train Loss: 0.3089 Acc: 89.42%\n",
      "Epoch 16/100 | Train Loss: 0.3000 Acc: 89.63%\n",
      "Epoch 17/100 | Train Loss: 0.2950 Acc: 89.95%\n",
      "Epoch 18/100 | Train Loss: 0.2871 Acc: 90.08%\n",
      "Epoch 19/100 | Train Loss: 0.2804 Acc: 90.28%\n",
      "Epoch 20/100 | Train Loss: 0.2676 Acc: 90.77%\n",
      "Epoch 21/100 | Train Loss: 0.2593 Acc: 91.07%\n",
      "Epoch 22/100 | Train Loss: 0.2603 Acc: 90.99%\n",
      "Epoch 23/100 | Train Loss: 0.2531 Acc: 91.10%\n",
      "Epoch 24/100 | Train Loss: 0.2447 Acc: 91.60%\n",
      "Epoch 25/100 | Train Loss: 0.2413 Acc: 91.50%\n",
      "Epoch 26/100 | Train Loss: 0.2401 Acc: 91.76%\n",
      "Epoch 27/100 | Train Loss: 0.2353 Acc: 91.86%\n",
      "Epoch 28/100 | Train Loss: 0.2307 Acc: 91.94%\n",
      "Epoch 29/100 | Train Loss: 0.2274 Acc: 92.16%\n",
      "Epoch 30/100 | Train Loss: 0.2227 Acc: 92.29%\n",
      "Epoch 31/100 | Train Loss: 0.2207 Acc: 92.41%\n",
      "Epoch 32/100 | Train Loss: 0.2170 Acc: 92.59%\n",
      "Epoch 33/100 | Train Loss: 0.2186 Acc: 92.32%\n",
      "Epoch 34/100 | Train Loss: 0.2174 Acc: 92.36%\n",
      "Epoch 35/100 | Train Loss: 0.2074 Acc: 92.81%\n",
      "Epoch 36/100 | Train Loss: 0.2141 Acc: 92.53%\n",
      "Epoch 37/100 | Train Loss: 0.2075 Acc: 92.86%\n",
      "Epoch 38/100 | Train Loss: 0.2040 Acc: 92.92%\n",
      "Epoch 39/100 | Train Loss: 0.1987 Acc: 93.07%\n",
      "Epoch 40/100 | Train Loss: 0.2024 Acc: 92.99%\n",
      "Epoch 41/100 | Train Loss: 0.2042 Acc: 92.87%\n",
      "Epoch 42/100 | Train Loss: 0.1961 Acc: 93.10%\n",
      "Epoch 43/100 | Train Loss: 0.1989 Acc: 93.13%\n",
      "Epoch 44/100 | Train Loss: 0.2008 Acc: 93.03%\n",
      "Epoch 45/100 | Train Loss: 0.1948 Acc: 93.20%\n",
      "Epoch 46/100 | Train Loss: 0.1922 Acc: 93.37%\n",
      "Epoch 47/100 | Train Loss: 0.1895 Acc: 93.34%\n",
      "Epoch 48/100 | Train Loss: 0.1911 Acc: 93.37%\n",
      "Epoch 49/100 | Train Loss: 0.1903 Acc: 93.34%\n",
      "Epoch 50/100 | Train Loss: 0.1881 Acc: 93.52%\n",
      "Epoch 51/100 | Train Loss: 0.1927 Acc: 93.20%\n",
      "Epoch 52/100 | Train Loss: 0.1823 Acc: 93.63%\n",
      "Epoch 53/100 | Train Loss: 0.1871 Acc: 93.48%\n",
      "Epoch 54/100 | Train Loss: 0.1842 Acc: 93.69%\n",
      "Epoch 55/100 | Train Loss: 0.1866 Acc: 93.38%\n",
      "Epoch 56/100 | Train Loss: 0.1842 Acc: 93.55%\n",
      "Epoch 57/100 | Train Loss: 0.1834 Acc: 93.47%\n",
      "Epoch 58/100 | Train Loss: 0.1869 Acc: 93.50%\n",
      "Epoch 59/100 | Train Loss: 0.1807 Acc: 93.71%\n",
      "Epoch 60/100 | Train Loss: 0.1783 Acc: 93.69%\n",
      "Epoch 61/100 | Train Loss: 0.1783 Acc: 93.68%\n",
      "Epoch 62/100 | Train Loss: 0.1833 Acc: 93.69%\n",
      "Epoch 63/100 | Train Loss: 0.1805 Acc: 93.73%\n",
      "Epoch 64/100 | Train Loss: 0.1784 Acc: 93.77%\n",
      "Epoch 65/100 | Train Loss: 0.1827 Acc: 93.68%\n",
      "Epoch 66/100 | Train Loss: 0.1797 Acc: 93.79%\n",
      "Epoch 67/100 | Train Loss: 0.1762 Acc: 93.83%\n",
      "Epoch 68/100 | Train Loss: 0.1800 Acc: 93.75%\n",
      "Epoch 69/100 | Train Loss: 0.1781 Acc: 93.83%\n",
      "Epoch 70/100 | Train Loss: 0.1758 Acc: 93.88%\n",
      "Epoch 71/100 | Train Loss: 0.1761 Acc: 93.73%\n",
      "Epoch 72/100 | Train Loss: 0.1756 Acc: 93.84%\n",
      "Epoch 73/100 | Train Loss: 0.1721 Acc: 94.06%\n",
      "Epoch 74/100 | Train Loss: 0.1706 Acc: 93.99%\n",
      "Epoch 75/100 | Train Loss: 0.1790 Acc: 93.77%\n",
      "Epoch 76/100 | Train Loss: 0.1699 Acc: 94.04%\n",
      "Epoch 77/100 | Train Loss: 0.1724 Acc: 93.98%\n",
      "Epoch 78/100 | Train Loss: 0.1689 Acc: 94.10%\n",
      "Epoch 79/100 | Train Loss: 0.1684 Acc: 94.05%\n",
      "Epoch 80/100 | Train Loss: 0.1693 Acc: 94.19%\n",
      "Epoch 81/100 | Train Loss: 0.1732 Acc: 93.95%\n",
      "Epoch 82/100 | Train Loss: 0.1695 Acc: 94.01%\n",
      "Epoch 83/100 | Train Loss: 0.1747 Acc: 94.00%\n",
      "Epoch 84/100 | Train Loss: 0.1689 Acc: 94.06%\n",
      "Epoch 85/100 | Train Loss: 0.1697 Acc: 94.17%\n",
      "Epoch 86/100 | Train Loss: 0.1711 Acc: 94.04%\n",
      "Epoch 87/100 | Train Loss: 0.1659 Acc: 94.23%\n",
      "Epoch 88/100 | Train Loss: 0.1737 Acc: 93.87%\n",
      "Epoch 89/100 | Train Loss: 0.1713 Acc: 94.05%\n",
      "Epoch 90/100 | Train Loss: 0.1696 Acc: 94.03%\n",
      "Epoch 91/100 | Train Loss: 0.1666 Acc: 94.02%\n",
      "Epoch 92/100 | Train Loss: 0.1680 Acc: 94.15%\n",
      "Epoch 93/100 | Train Loss: 0.1671 Acc: 94.28%\n",
      "Epoch 94/100 | Train Loss: 0.1681 Acc: 94.18%\n",
      "Epoch 95/100 | Train Loss: 0.1640 Acc: 94.28%\n",
      "Epoch 96/100 | Train Loss: 0.1675 Acc: 94.14%\n",
      "Epoch 97/100 | Train Loss: 0.1666 Acc: 94.19%\n",
      "Epoch 98/100 | Train Loss: 0.1662 Acc: 94.20%\n",
      "Epoch 99/100 | Train Loss: 0.1663 Acc: 94.10%\n",
      "Epoch 100/100 | Train Loss: 0.1658 Acc: 94.20%\n",
      "\n",
      "Saved figures:\n",
      " - cifar_loss.png\n",
      " - cifar_train_acc.png\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce50655-d528-46bc-87c8-92e48ee89e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
